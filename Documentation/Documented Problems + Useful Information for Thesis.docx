os.environ["CUDA_VISIBLE_DEVICES"] = "-1" 

==========================================

Some model information for later use:

VGG16:

Accuracy:  0.9258
Recall:  0.9028
Precision:  0.9846220961936961
Average Precision:  0.9537168284436689

F-1 score: 0.08844714029590009 ??????????????? Presentation score on old pc: 0.927
Accuracy:  0.46626666666666666
Recall:  0.6162
Precision:  0.33606020942408377
Average Precision Score:  0.33501363438045373

with Shuffle = False & 'binary' class mode

Accuracy:  0.07413333333333333
Recall:  0.028
Precision:  0.015270506108202443
Average Precision Score:  0.32442757417102963

==========================================

Keras Sequential (Original):

Accuracy:  0.950625
Recall:  0.9649662
Precision:  0.93955714
Average Precision Score:  0.9932718185313343
F1 Score: 0.9411764705882353

==========================================

Keras Sequential v2 (Spatial Attention Included):

Accuracy: 0.9809375
Recall:  0.9584121
Precision: 0.9795031
Average Precision Score:  0.9945054945054944
F1 Score: 1.0

Model is producing the same result on different predicition cases: array([0.58519393], dtype=float32), array([0.58519393], dtype=float32), array([0.58519393], dtype=float32), array([0.58519393], dtype=float32)

Even when presented with an infected or uninfected cells from malaria-nih: uninfected: [[0.58519393]], infected: [[0.58519393]]

==========================================

Some failure modes of segmentation:

One of the failure modes of the segmentation occurs when multiple cells are overlapping each other or extremely close proximity to each other, this causes multiple cells to be treated as one region of interest which causes the classifier to miss-identify the region of interest, this miss-identification can be a False-Positive or a False-Negative

The approach to fix this would be look at edge detection in more detail and identifying the light diffracted regions at the edges of the cells which would in theory be able to separate overlapping cells

Large thresholds are filtered out as large overlapping clusters of cells are identified and this is something we want to avoid, the classifier also miss-identifies these large thresholds as False-Positives

Smaller than cell regions of interest are also filtered out as these not of interest and are usually lighting artefacts that are identified by Otsu’s thresholding

K-means identifies small thresholds more than Otsu and they get through the contour filtering (on 4 clusters), it sometimes finds
light artifacts in the image as it is unable to differentiate. K-means preprocessing turns the images into LAB colour space, this
almost 'enhances' the contrast of the images before they are processed


==========================================

Why did we not port to mobile?

How computationally expensive would our segmentation and classification be on mobile?

Why is there only a limited amount of segmenters and classifier?

==========================================

How does an unknown database perform on our classifiers.

For instance if we take a plant cell database and use it on the classifier to identify its accuracy

	- Using the following http://cellimagelibrary.org/project/P2005 project's data, we test the classifiers
	and the end-to-end system and measure its accuracy on these images, then we comment on how this data can
	be perceived.

	- We can also try the MNIST database

==========================================

During the pre-processing data section for the classification, mention the attempt to use .taskfiles to create a different 
and improved dataset for training. Dhaka task file has been provided from Adrian F. Clark which contained revised labelling
of the NIH database. 

==========================================

Gooey GUI has been investigated, can be mentioned in the final product section of the thesis 

- User can choose segmentation methods and which model to use to evaluate the segmented images

==========================================

UKNOWN DATABASES PERFORMANCE:

keras og model predictions:
1/1 [==============================] - 0s 64ms/step
[[0.]]
1/1 [==============================] - 0s 18ms/step
[[0.]]
1/1 [==============================] - 0s 19ms/step
[[0.]]

=====================================
keras spatial model predictions:
1/1 [==============================] - 0s 88ms/step
[[9.700243e-17]]
1/1 [==============================] - 0s 23ms/step
[[0.5848045]]
1/1 [==============================] - 0s 24ms/step
[[0.5851939]]

=====================================
vgg16 model predictions:
1/1 [==============================] - 0s 195ms/step
[[0. 1.]]
1/1 [==============================] - 0s 102ms/step
[[0. 1.]]
1/1 [==============================] - 0s 99ms/step
[[0. 1.]]

=====================================
313/313 [==============================] - 5s 17ms/step
...
313/313 [==============================] - 821s 3s/step

Accuracy of first keras: 0.098
Accuracy of second keras: 0.098
Accuracy of vgg16: 0.1137